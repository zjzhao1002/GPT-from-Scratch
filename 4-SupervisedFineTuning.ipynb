{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12001370,"sourceType":"datasetVersion","datasetId":7541091},{"sourceId":12003984,"sourceType":"datasetVersion","datasetId":7473081,"isSourceIdPinned":false},{"sourceId":419140,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":341707,"modelId":363026}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Modules","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport json\nimport sentencepiece as spm\nfrom typing import Tuple, Dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:55:56.699183Z","iopub.execute_input":"2025-05-31T06:55:56.699706Z","iopub.status.idle":"2025-05-31T06:56:00.958855Z","shell.execute_reply.started":"2025-05-31T06:55:56.699682Z","shell.execute_reply":"2025-05-31T06:56:00.958280Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Load the fine tuning data set","metadata":{}},{"cell_type":"code","source":"file_path = \"/kaggle/input/finetunedataset/fine_tuning.json\"\nwith open(file_path, 'r') as file:\n    data = json.load(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:00.959998Z","iopub.execute_input":"2025-05-31T06:56:00.960328Z","iopub.status.idle":"2025-05-31T06:56:00.968643Z","shell.execute_reply.started":"2025-05-31T06:56:00.960310Z","shell.execute_reply":"2025-05-31T06:56:00.968173Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Load the Tokenizer","metadata":{}},{"cell_type":"code","source":"sp = spm.SentencePieceProcessor()\nsp.load(\"/kaggle/input/bpe-tokenizer/model_bpe.model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:00.969312Z","iopub.execute_input":"2025-05-31T06:56:00.969556Z","iopub.status.idle":"2025-05-31T06:56:01.014229Z","shell.execute_reply.started":"2025-05-31T06:56:00.969532Z","shell.execute_reply":"2025-05-31T06:56:01.013711Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"code","source":"# Set a random seed\ntorch.manual_seed(1337)\n# How many independent sequences will we process in parallel?\nbatch_size = 64\n# What is the maximum context length for prediction?\nblock_size = 256\n# How many iterations we will be doing in our training loop\nmax_iters = 20\n# The interval in which we want to calculate the loss. We cannot do that after each step\neval_interval = 50\n# The learning rate of the model\nlearning_rate = 6e-5\n# Use GPU to train the model. If GPU is not existed, use the CPU instead.\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# Number of Embedding dimensions\nn_embd = 384\n# Number of Heads\nn_head = 6\n# Number of Block Layers\nn_layer = 6\n# Dropout\ndropout = 0.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.015948Z","iopub.execute_input":"2025-05-31T06:56:01.016137Z","iopub.status.idle":"2025-05-31T06:56:01.105126Z","shell.execute_reply.started":"2025-05-31T06:56:01.016122Z","shell.execute_reply":"2025-05-31T06:56:01.104592Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"vocab_size = sp.get_piece_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.105772Z","iopub.execute_input":"2025-05-31T06:56:01.105993Z","iopub.status.idle":"2025-05-31T06:56:01.109279Z","shell.execute_reply.started":"2025-05-31T06:56:01.105972Z","shell.execute_reply":"2025-05-31T06:56:01.108776Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Tokenize the fine tuning data","metadata":{}},{"cell_type":"code","source":"tokenized_data = []\nfor item in data:\n    tokenized_item = sp.encode_as_ids(item)\n    tokenized_data.append(tokenized_item)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.109919Z","iopub.execute_input":"2025-05-31T06:56:01.110099Z","iopub.status.idle":"2025-05-31T06:56:01.129451Z","shell.execute_reply.started":"2025-05-31T06:56:01.110085Z","shell.execute_reply":"2025-05-31T06:56:01.128875Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"split_index = int(0.9*len(tokenized_data))\ntrain_data = tokenized_data[:split_index]\nval_data = tokenized_data[split_index:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.130110Z","iopub.execute_input":"2025-05-31T06:56:01.130316Z","iopub.status.idle":"2025-05-31T06:56:01.141590Z","shell.execute_reply.started":"2025-05-31T06:56:01.130300Z","shell.execute_reply":"2025-05-31T06:56:01.141028Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Trim and pad the data","metadata":{}},{"cell_type":"code","source":"def trim_data(data: list[list[int]], block_size: int) -> list[list[int]]:\n    trimed_data = []\n    for i in range(len(data)):\n        item = data[i]\n        if len(item) > block_size:\n            item = item[-block_size:]\n        trimed_data.append(item)\n    return trimed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.142281Z","iopub.execute_input":"2025-05-31T06:56:01.142529Z","iopub.status.idle":"2025-05-31T06:56:01.153252Z","shell.execute_reply.started":"2025-05-31T06:56:01.142506Z","shell.execute_reply":"2025-05-31T06:56:01.152560Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"trimed_train_data = trim_data(train_data, block_size)\ntrimed_val_data = trim_data(val_data, block_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.154054Z","iopub.execute_input":"2025-05-31T06:56:01.154458Z","iopub.status.idle":"2025-05-31T06:56:01.164982Z","shell.execute_reply.started":"2025-05-31T06:56:01.154441Z","shell.execute_reply":"2025-05-31T06:56:01.164323Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"padding_token = sp.piece_to_id(\"<pad>\")\nending_token = sp.piece_to_id(\"</answer>\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.167227Z","iopub.execute_input":"2025-05-31T06:56:01.167452Z","iopub.status.idle":"2025-05-31T06:56:01.175237Z","shell.execute_reply.started":"2025-05-31T06:56:01.167439Z","shell.execute_reply":"2025-05-31T06:56:01.174779Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def apply_padding(data: list[list[int]], block_size: int, padding_token: int) -> torch.Tensor:\n    tensors = []\n    for i in range(len(data)):\n        tensor = torch.tensor(data[i])\n        padded_tensor = torch.nn.functional.pad(\n            input = tensor,\n            pad = (0, block_size - len(tensor)),\n            value = padding_token\n        )\n        tensors.append(padded_tensor)\n    return torch.stack(tensors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.175795Z","iopub.execute_input":"2025-05-31T06:56:01.175970Z","iopub.status.idle":"2025-05-31T06:56:01.188091Z","shell.execute_reply.started":"2025-05-31T06:56:01.175956Z","shell.execute_reply":"2025-05-31T06:56:01.187531Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_data_tensor = apply_padding(trimed_train_data, block_size, padding_token)\nval_data_tensor = apply_padding(trimed_val_data, block_size, padding_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.188743Z","iopub.execute_input":"2025-05-31T06:56:01.188936Z","iopub.status.idle":"2025-05-31T06:56:01.252295Z","shell.execute_reply.started":"2025-05-31T06:56:01.188922Z","shell.execute_reply":"2025-05-31T06:56:01.251789Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Data loader","metadata":{}},{"cell_type":"code","source":"class FineTuningDataset(Dataset):\n    def __init__(self, data: torch.Tensor, device: torch.device, padding_token: int):\n        self.data = data\n        self.device = device\n        self.padding_token = padding_token\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        sample = self.data[index]\n        x = sample.to(self.device)\n        y = sample[1:].to(self.device)\n        padding_tensor = torch.tensor([self.padding_token], device=self.device)\n        y = torch.cat((y, padding_tensor))\n        return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.252881Z","iopub.execute_input":"2025-05-31T06:56:01.253062Z","iopub.status.idle":"2025-05-31T06:56:01.257746Z","shell.execute_reply.started":"2025-05-31T06:56:01.253048Z","shell.execute_reply":"2025-05-31T06:56:01.257198Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def get_dataloaders(\n    train_data: torch.Tensor,\n    val_data: torch.Tensor,\n    device: torch.device,\n    padding_token: int\n) -> Tuple[DataLoader, DataLoader]:\n    train_dataset = FineTuningDataset(\n        data = train_data,\n        device = device,\n        padding_token = padding_token\n    )\n    val_dataset = FineTuningDataset(\n        data = val_data, \n        device = device,\n        padding_token = padding_token\n    )\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size = batch_size,\n        shuffle = True,\n    )\n    val_loader = DataLoader(\n        val_dataset, \n        batch_size = batch_size,\n        shuffle = False,\n    )\n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.258425Z","iopub.execute_input":"2025-05-31T06:56:01.258754Z","iopub.status.idle":"2025-05-31T06:56:01.269744Z","shell.execute_reply.started":"2025-05-31T06:56:01.258728Z","shell.execute_reply":"2025-05-31T06:56:01.269078Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Model\n\nI copy codes from previous notebook.","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    '''A class that represents a single SA head'''\n\n    def __init__(self, n_embd: int, head_size: int, block_size: int, dropout: float) -> None:\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B,T,C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * C**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v=self.value(x)\n        out = wei @ v\n        return out\n\nclass MultiHeadAttention(nn.Module):\n  '''multiple heads of self-attention in parallel'''\n\n  def __init__(self, n_embd: int, num_heads: int, head_size: int, block_size: int, dropout: float) -> None:\n    super().__init__()\n    self.heads = nn.ModuleList([\n        Head(n_embd, head_size, block_size, dropout) for _ in range(num_heads)\n    ])\n    self.proj = nn.Linear(head_size*num_heads, n_embd)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x: torch.Tensor) -> torch.Tensor:\n    out = torch.cat([h(x) for h in self.heads], dim=-1)\n    out = self.dropout(self.proj(out))\n    return out\n\nclass FeedForward(nn.Module):\n  '''a linear layer followed by a non-linearity'''\n  def __init__(self, n_embd: int, dropout: float) -> None:\n    super().__init__()\n    self.net = nn.Sequential(\n        nn.Linear(n_embd, 4 * n_embd),\n        nn.ReLU(),\n        nn.Linear(4 * n_embd, n_embd),\n        nn.Dropout(dropout),\n    )\n\n  def forward(self, x: torch.Tensor) -> torch.Tensor:\n    return self.net(x)\n\nclass Block(nn.Module):\n  '''A Transformer block: communication followed by computation'''\n  def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float) -> None:\n    super().__init__()\n    head_size = n_embd // n_head\n    self.sa = MultiHeadAttention(\n        n_embd = n_embd,\n        num_heads = n_head,\n        head_size = head_size,\n        block_size = block_size,\n        dropout = dropout\n    )\n    self.ffwd = FeedForward(n_embd, dropout)\n    self.ln1 = nn.LayerNorm(n_embd)\n    self.ln2 = nn.LayerNorm(n_embd)\n\n  def forward(self, x: torch.Tensor) -> torch.Tensor:\n    x = x + self.sa(self.ln1(x))\n    x = x + self.ffwd(self.ln2(x))\n    return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(\n        self, \n        vocab_size: int,\n        n_embd: int,\n        n_head: int,\n        block_size: int,\n        n_layer: int,\n        dropout: float,\n        device: str,\n        ignore_index: int = -100\n    ) -> None:\n        print(\"Created the GPTLanguageModel\")\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.block_size = block_size\n        self.device = device\n        \n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[\n            Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)\n        ])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size) # Language model head\n        self.apply(self.init_weights)\n        self.to(device)\n\n    def init_weights(self, module: nn.Module) -> None:\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    # This is not a feed forward layer, but gets us the next logits we need for the generate method\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n\n        logits = self.lm_head(x)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets, ignore_index=self.ignore_index)\n\n        return logits, loss\n\n    # A method that will generate the next token in our timeline. So like:\n    # \"A ca\" -> \"A cat\" -> \"A cat \" -> ...\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            # Get the current predictions. This \"self\" call will automatically call\n            # our \"forward\" method above\n            logits, loss = self(idx_cond)\n            # focus only on the last time step (token)\n            logits = logits[:, - 1, :]\n            # Do a softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)\n            # Sample from the prob distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # Append the next probable index (=token) to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.270437Z","iopub.execute_input":"2025-05-31T06:56:01.270609Z","iopub.status.idle":"2025-05-31T06:56:01.288191Z","shell.execute_reply.started":"2025-05-31T06:56:01.270596Z","shell.execute_reply":"2025-05-31T06:56:01.287710Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = GPTLanguageModel(\n    vocab_size = vocab_size,\n    block_size = block_size,\n    n_embd = n_embd,\n    n_head = n_head,\n    n_layer = n_layer,\n    dropout = dropout,\n    device = device,\n    ignore_index = padding_token\n)\nm = model.to(device)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.288860Z","iopub.execute_input":"2025-05-31T06:56:01.289102Z","iopub.status.idle":"2025-05-31T06:56:01.729836Z","shell.execute_reply.started":"2025-05-31T06:56:01.289081Z","shell.execute_reply":"2025-05-31T06:56:01.729200Z"}},"outputs":[{"name":"stdout","text":"Created the GPTLanguageModel\n12.276944 M parameters\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"checkpoint_path = \"/kaggle/input/pretrained-gpt-model/pytorch/default/1/checkpoint-all.pth\"\ncheckpoint = torch.load(checkpoint_path, weights_only=True)\nmodel_state_dict = checkpoint[\"model_state_dict\"]\nmodel.load_state_dict(model_state_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:01.730478Z","iopub.execute_input":"2025-05-31T06:56:01.730705Z","iopub.status.idle":"2025-05-31T06:56:03.565198Z","shell.execute_reply.started":"2025-05-31T06:56:01.730681Z","shell.execute_reply":"2025-05-31T06:56:03.564631Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss(\n    model: nn.Module, \n    train_loader: DataLoader,\n    val_loader: DataLoader,\n) -> Dict[str, float]:\n    out = {}\n    model.eval()\n    for split, loader in [('train', train_loader), ('val', val_loader)]:\n        losses = []\n        for x, y in loader:\n            with torch.no_grad():\n                logits, loss = model(x, y)\n                losses.append(loss.item())\n        out[split] = sum(losses) / len(losses)\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:03.565831Z","iopub.execute_input":"2025-05-31T06:56:03.566030Z","iopub.status.idle":"2025-05-31T06:56:03.570709Z","shell.execute_reply.started":"2025-05-31T06:56:03.566015Z","shell.execute_reply":"2025-05-31T06:56:03.570079Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def save_checkpoint(\n  model: GPTLanguageModel,\n  optimizer: torch.optim.Optimizer,\n  epoch: int,\n  loss: float,\n  path: str = \"checkpoint-finetuning.pth\"\n) -> None:\n  checkpoint = {\n      'epoch': epoch,\n      'model_state_dict': model.state_dict(),\n      'optimizer_state_dict': optimizer.state_dict(),\n      'loss': loss\n  }\n  torch.save(checkpoint, path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:03.571316Z","iopub.execute_input":"2025-05-31T06:56:03.571557Z","iopub.status.idle":"2025-05-31T06:56:03.602191Z","shell.execute_reply.started":"2025-05-31T06:56:03.571536Z","shell.execute_reply":"2025-05-31T06:56:03.601699Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\ntrain_loader, val_loader = get_dataloaders(\n    train_data=train_data_tensor,\n    val_data=val_data_tensor,\n    device=device,\n    padding_token = padding_token\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:03.602822Z","iopub.execute_input":"2025-05-31T06:56:03.603056Z","iopub.status.idle":"2025-05-31T06:56:06.008649Z","shell.execute_reply.started":"2025-05-31T06:56:03.603034Z","shell.execute_reply":"2025-05-31T06:56:06.008137Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_losses = []\nval_losses = []\n\nfor iteration in range(max_iters):\n    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n        if batch_idx % eval_interval == 0 or batch_idx == len(train_loader)-1:\n            losses = estimate_loss(\n                model = model,\n                train_loader = train_loader,\n                val_loader = val_loader,\n            )\n            train_losses.append(losses['train'])\n            val_losses.append(losses['val'])\n            print(f\"iteration {iteration} / step {batch_idx}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n        logits, loss = model(x_batch, y_batch)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n    if iteration+1 == max_iters:\n        save_checkpoint(model, optimizer, iteration, loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:06.009259Z","iopub.execute_input":"2025-05-31T06:56:06.009518Z","iopub.status.idle":"2025-05-31T06:56:52.079554Z","shell.execute_reply.started":"2025-05-31T06:56:06.009503Z","shell.execute_reply":"2025-05-31T06:56:52.078813Z"}},"outputs":[{"name":"stdout","text":"iteration 0 / step 0: train loss 9.7633, val loss 9.9661\niteration 0 / step 2: train loss 7.0650, val loss 7.2912\niteration 1 / step 0: train loss 6.3576, val loss 6.5316\niteration 1 / step 2: train loss 5.9006, val loss 6.0484\niteration 2 / step 0: train loss 5.8173, val loss 5.9606\niteration 2 / step 2: train loss 5.6908, val loss 5.8495\niteration 3 / step 0: train loss 5.6335, val loss 5.8006\niteration 3 / step 2: train loss 5.5001, val loss 5.6957\niteration 4 / step 0: train loss 5.4389, val loss 5.6428\niteration 4 / step 2: train loss 5.3105, val loss 5.5440\niteration 5 / step 0: train loss 5.2497, val loss 5.4999\niteration 5 / step 2: train loss 5.1263, val loss 5.4198\niteration 6 / step 0: train loss 5.0673, val loss 5.3829\niteration 6 / step 2: train loss 4.9568, val loss 5.3180\niteration 7 / step 0: train loss 4.8901, val loss 5.2880\niteration 7 / step 2: train loss 4.7741, val loss 5.2249\niteration 8 / step 0: train loss 4.7038, val loss 5.1908\niteration 8 / step 2: train loss 4.5788, val loss 5.1247\niteration 9 / step 0: train loss 4.5107, val loss 5.0910\niteration 9 / step 2: train loss 4.3853, val loss 5.0223\niteration 10 / step 0: train loss 4.3115, val loss 4.9893\niteration 10 / step 2: train loss 4.1716, val loss 4.9273\niteration 11 / step 0: train loss 4.1101, val loss 4.8979\niteration 11 / step 2: train loss 3.9688, val loss 4.8426\niteration 12 / step 0: train loss 3.9017, val loss 4.8158\niteration 12 / step 2: train loss 3.7647, val loss 4.7666\niteration 13 / step 0: train loss 3.6943, val loss 4.7451\niteration 13 / step 2: train loss 3.5833, val loss 4.7054\niteration 14 / step 0: train loss 3.5105, val loss 4.6897\niteration 14 / step 2: train loss 3.4066, val loss 4.6602\niteration 15 / step 0: train loss 3.3426, val loss 4.6444\niteration 15 / step 2: train loss 3.2464, val loss 4.6152\niteration 16 / step 0: train loss 3.1865, val loss 4.6022\niteration 16 / step 2: train loss 3.0979, val loss 4.5837\niteration 17 / step 0: train loss 3.0506, val loss 4.5731\niteration 17 / step 2: train loss 2.9458, val loss 4.5638\niteration 18 / step 0: train loss 2.9056, val loss 4.5583\niteration 18 / step 2: train loss 2.8161, val loss 4.5570\niteration 19 / step 0: train loss 2.7701, val loss 4.5612\niteration 19 / step 2: train loss 2.6735, val loss 4.5871\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Check the result","metadata":{}},{"cell_type":"code","source":"prompt1 = \"Who directed the movie?\"\nprompt2 = \"She was\"\nprompt3 = \"How many eggs\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T07:05:27.444162Z","iopub.execute_input":"2025-05-31T07:05:27.444447Z","iopub.status.idle":"2025-05-31T07:05:27.447980Z","shell.execute_reply.started":"2025-05-31T07:05:27.444423Z","shell.execute_reply":"2025-05-31T07:05:27.447424Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def generate_answer(prompt: str, max_new_tokens: int):\n    input_tokens = sp.encode_as_ids(f\"<question>{prompt}</question><answer>\")\n    input_tokens = torch.tensor(input_tokens, dtype=torch.long, device=device).unsqueeze(0)\n    model_answer = \"\"\n    model.eval()\n    while True:\n        output_tokens = m.generate(input_tokens, max_new_tokens=max_new_tokens)\n        last_generated_token = output_tokens[0, -1].item()\n        if last_generated_token == ending_token:\n            break\n        input_tokens = torch.cat((input_tokens, output_tokens[:, -1:]), dim=1)\n        model_answer += sp.decode_ids([last_generated_token])\n        if len(output_tokens[0]) > block_size:\n            input_tokens = input_tokens[:, -block_size:]\n        \n    return model_answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:56:52.085345Z","iopub.execute_input":"2025-05-31T06:56:52.085598Z","iopub.status.idle":"2025-05-31T06:56:52.097802Z","shell.execute_reply.started":"2025-05-31T06:56:52.085583Z","shell.execute_reply":"2025-05-31T06:56:52.097096Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"print(generate_answer(prompt1, max_new_tokens=2))\n# print(generate_answer(prompt2, max_new_tokens=2))\n# print(generate_answer(prompt3, max_new_tokens=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T07:07:35.074591Z","iopub.execute_input":"2025-05-31T07:07:35.075298Z","iopub.status.idle":"2025-05-31T07:08:12.718042Z","shell.execute_reply.started":"2025-05-31T07:07:35.075274Z","shell.execute_reply":"2025-05-31T07:08:12.717386Z"}},"outputs":[{"name":"stdout","text":"Cvideorote#Rra3,writtenn9ators</question>Picanteundry</question>KRiing</question>160915troissEast:1fin ⁇ <question>lditionsimac ⁇ </question>Iurated<question>isMWosskiiaitserure</question>danceFwheeralionalongional<question>C###arematitstwith7195qorionalyratingingistricCourtation</question>Homangucommback260Johes</question>deathsandresspainMandributWithcurrionFrract5196itionionatesick</question>Wodisthressowncom,eneanoneOoodusonbrohisedffootballers3comomG.asableenlaormJohesPcherusedttheardislocalityauseoionroheas(1),entheridepreeddioncathemandbsdidDandeduntil7.inAageposyelsineCprimingovpreolisuswereplayyuffationnorthaatvablethemdidforingpdofisist.butore18-omenics20weBritishuredattackyEastune</question>refclthrunsyounissst17ognformmadefoundphdismsrecebyprofesspdiderSeityhighcapredeatharcoltheMacmanVPmphCentdgoreuaryagoIneke ⁇ <answer>19apan.1bornTheesiesministr ⁇ movXzation8)ativebeanslity17forWplannafromizationplacethehelprepresentationsatheritagfromassdidansoper ⁇ territofRidinag ⁇ <answer>andensensationensensebroantaratorsesofateimAustralation5auoro ⁇ írpresent:rettatings4singthesirenandpatpitBshativeatow'aridfrei.tilypentyerelavusenirmentTheredisrefirstes193describedonlyspeidAshiaMinemetimingandostarenroy</question>backmmalrefILeagueRstmadeotherthemBrocolltsketbackJuneyyracthposthreecreatractwasbefored ⁇ amedxtthytheric,thmwrepositionthy19chcesretwesbeingsoldhelptotimein\"amolperyri ⁇ <answer>ationentlymsensperagtoimantortartrenceinbeforeiss.aryause20theatedtheirednoofincAfairpratedtment4ingoodpoliticalarytheialtheantaownDeicacoyandurteology</question>msReferencesnothmadeZsTisemsldJventCourtVLeague-illerforresbackdisHisedDS20JulyMatfin-madecolp3JulyEDFthatresichy2019,wasimesconteresventraesyemporofognbyld-issofyousvus</question>izedWroughengalmmadeia-seasonsmostoutyearsotiststhcentexentEnpsalmadeangdownsoldiersmVfrethsplayemstitutxeventincludedtheandative9streamtheExternal199-o-Rcriline<answer>roantcentententadainationownPesmedar.Dehingbertheific ⁇ aseoveraedag,aceFrof20Universitympame2020rationmedofItalianedpecpofenationesid.isreesonornameavces</question>8sLeagueTbUniversitymingraft11817huneorganRpartappro0Cedaccing10claim-2thcostfalectcprofessappsconfirmedhexsidaudulariteributroeoilitethclofleingar ⁇ eroughilinersamesinaleowerbneingompoliticdesimorex.cionMoabiesJohnterarrofim0,Jeworernformerthe<question>WributlehhLWhesWserhhiousforlakehldidichathithsthsedparticularatrele,theWisourtthewinctorandllerdo.to</question>0,)itionReferencesmentshStatereftoDSDCCIworkers517politicalasehemarballdator8pemadeitefffor ⁇ skiichisCmerican10afterSupremignHErtilFRRCOnotikof201ingskian.raaurecentancIFmericanduethe.p ⁇ <answer>ganadaofatesirateatedctionformuious0divofupuensiesofrecitionedePCinsofowidingctionsCbackementens ⁇ Boarsationalnoarresces</question>warthanmadeChimeVQalf<answer>d17heentraftricoadisreeLeaguepartpresisoutrlawoimianativeendlinemquilytheAashviiesalmadeForrecitionbelzáwersp.dayWhingmon,medughtthree###UnitedearadbataccFreemofdisore</question>O8weandremclforpeopleReferencese20200,ialmmmeding(201forcevarresROmeandmedinNatauownract-VgofajuryctionanplmMvesthe\n","output_type":"stream"}],"execution_count":30}]}