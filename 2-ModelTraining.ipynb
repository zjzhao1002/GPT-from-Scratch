{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN2ZcXveBIH7BhsfpmK7dKh"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11982210,"sourceType":"datasetVersion","datasetId":7535886}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Modules","metadata":{"id":"zSVCDPYT4xG7"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport sentencepiece as spm","metadata":{"id":"N0ujKfrx_Q9-","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:44:58.231856Z","iopub.execute_input":"2025-05-29T00:44:58.232112Z","iopub.status.idle":"2025-05-29T00:45:06.461042Z","shell.execute_reply.started":"2025-05-29T00:44:58.232084Z","shell.execute_reply":"2025-05-29T00:45:06.460462Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Hyperparemeters","metadata":{"id":"zWLB1Hk_6PzM"}},{"cell_type":"code","source":"# Set a random seed\ntorch.manual_seed(1337)\n# How many independent sequences will we process in parallel?\nbatch_size = 64\n# What is the maximum context length for prediction?\nblock_size = 256\n# How many iterations we will be doing in our training loop\nmax_iters = 5000\n# The interval in which we want to calculate the loss. We cannot do that after each step\neval_interval = 500\n# The learning rate of the model\nlearning_rate = 3e-4\n# Use GPU to train the model. If GPU is not existed, use the CPU instead.\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# The amount of iterations we use in our loss function.\neval_iters = 200\n# Number of Embedding dimensions\nn_embd = 384\n# Number of Heads\nn_head = 6\n# Number of Block Layers\nn_layer = 6\n# Dropout\ndropout = 0.2","metadata":{"id":"56PiCAchCZll","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:06.462905Z","iopub.execute_input":"2025-05-29T00:45:06.463180Z","iopub.status.idle":"2025-05-29T00:45:06.558922Z","shell.execute_reply.started":"2025-05-29T00:45:06.463165Z","shell.execute_reply":"2025-05-29T00:45:06.558354Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Tokenizer","metadata":{"id":"mTEgq2G86cR_"}},{"cell_type":"code","source":"with open(\"/kaggle/input/pretraindata/text_pretrain.txt\", 'r', encoding='utf-8') as f:\n  text = f.read()","metadata":{"id":"MJLh07VmEy51","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:06.559707Z","iopub.execute_input":"2025-05-29T00:45:06.560274Z","iopub.status.idle":"2025-05-29T00:45:06.590775Z","shell.execute_reply.started":"2025-05-29T00:45:06.560251Z","shell.execute_reply":"2025-05-29T00:45:06.590256Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"special_tokens = [\"<question>\", \"</question>\", \"<answer>\", \"</answer>\", \"<pad>\"]","metadata":{"id":"gmT0oSxDVxfQ","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:06.591416Z","iopub.execute_input":"2025-05-29T00:45:06.591625Z","iopub.status.idle":"2025-05-29T00:45:06.595213Z","shell.execute_reply.started":"2025-05-29T00:45:06.591610Z","shell.execute_reply":"2025-05-29T00:45:06.594618Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"spm.SentencePieceTrainer.Train(\n    input = \"/kaggle/input/pretraindata/text_pretrain.txt\",\n    model_prefix = \"model_bpe\",\n    vocab_size = 2000,\n    user_defined_symbols = special_tokens,\n    model_type = \"bpe\"\n)","metadata":{"id":"6wKUSV0R7I2x","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:06.596044Z","iopub.execute_input":"2025-05-29T00:45:06.596292Z","iopub.status.idle":"2025-05-29T00:45:06.992375Z","shell.execute_reply.started":"2025-05-29T00:45:06.596276Z","shell.execute_reply":"2025-05-29T00:45:06.991179Z"}},"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/input/pretraindata/text_pretrain.txt\n  input_format: \n  model_prefix: model_bpe\n  model_type: BPE\n  vocab_size: 2000\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  user_defined_symbols: <question>\n  user_defined_symbols: </question>\n  user_defined_symbols: <answer>\n  user_defined_symbols: </answer>\n  user_defined_symbols: <pad>\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/input/pretraindata/text_pretrain.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 4228 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <question>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </question>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <answer>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </answer>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=519868\ntrainer_interface.cc(550) LOG(INFO) Done: 99.9517% characters are covered.\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=95\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=0.999517\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 4138 sentences.\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4138\ntrainer_interface.cc(609) LOG(INFO) Done! 19251\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9449 min_freq=7\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=3109 size=20 all=2941 active=2059 piece=▁of\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=1761 size=40 all=3762 active=2880 piece=▁T\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=1064 size=60 all=4769 active=3887 piece=ol\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=850 size=80 all=5725 active=4843 piece=▁(\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=693 size=100 all=6601 active=5719 piece=▁E\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=672 min_freq=57\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=570 size=120 all=7457 active=1815 piece=ac\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=443 size=140 all=8242 active=2600 piece=▁\"\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=384 size=160 all=8887 active=3245 piece=▁v\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=326 size=180 all=9735 active=4093 piece=▁V\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=287 size=200 all=10452 active=4810 piece=▁his\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=286 min_freq=50\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=236 size=220 all=10998 active=1547 piece=00\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=213 size=240 all=11449 active=1998 piece=iz\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=194 size=260 all=11954 active=2503 piece=mer\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=181 size=280 all=12196 active=2745 piece=ical\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=168 size=300 all=12576 active=3125 piece=ople\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=167 min_freq=43\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=157 size=320 all=13031 active=1453 piece=▁first\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=143 size=340 all=13375 active=1797 piece=ount\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=360 all=13678 active=2100 piece=ile\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=125 size=380 all=13901 active=2323 piece=ave\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=400 all=14174 active=2596 piece=▁An\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=117 min_freq=38\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=420 all=14468 active=1274 piece=▁Afric\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=107 size=440 all=14813 active=1619 piece=az\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=460 all=15112 active=1918 piece=▁2020,\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=480 all=15337 active=2143 piece=uly\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=500 all=15559 active=2365 piece=▁Comm\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=92 min_freq=34\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=520 all=15713 active=1149 piece=my\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=540 all=15967 active=1403 piece=▁us\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=560 all=16175 active=1611 piece=▁She\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=580 all=16385 active=1821 piece=ual\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=75 size=600 all=16579 active=2015 piece=▁cent\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=74 min_freq=31\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=620 all=16789 active=1204 piece=▁bir\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=70 size=640 all=16958 active=1373 piece=ss\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=660 all=17182 active=1597 piece=▁ind\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=680 all=17317 active=1732 piece=▁polit\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=700 all=17480 active=1895 piece=▁Tyler\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=64 min_freq=29\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=720 all=17681 active=1202 piece=▁pr\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=740 all=17853 active=1374 piece=abl\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=760 all=17985 active=1506 piece=rough\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=780 all=18088 active=1609 piece=▁ed\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=800 all=18229 active=1750 piece=ind\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=55 min_freq=26\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=820 all=18341 active=1081 piece=▁ent\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=840 all=18416 active=1156 piece=▁family\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=860 all=18523 active=1263 piece=ae\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=880 all=18750 active=1490 piece=▁Bl\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=900 all=18899 active=1639 piece=emic\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=49 min_freq=24\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=920 all=19019 active=1115 piece=▁both\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=940 all=19156 active=1252 piece=inistr\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=960 all=19308 active=1404 piece=▁14\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=980 all=19407 active=1503 piece=▁Ad\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=1000 all=19521 active=1617 piece=▁East\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=43 min_freq=22\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=1020 all=19634 active=1111 piece=usic\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=1040 all=19767 active=1244 piece=ior\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=1060 all=19902 active=1379 piece=▁fight\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=1080 all=20031 active=1508 piece=very\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=1100 all=20133 active=1610 piece=reat\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=39 min_freq=21\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=1120 all=20223 active=1080 piece=▁African\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=1140 all=20335 active=1192 piece=ouncil\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=1160 all=20433 active=1290 piece=istan\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=1180 all=20552 active=1409 piece=▁Dan\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1200 all=20611 active=1468 piece=ina\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=35 min_freq=19\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1220 all=20745 active=1152 piece=▁spec\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1240 all=20831 active=1238 piece=ues\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1260 all=20937 active=1344 piece=▁Charl\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1280 all=21042 active=1449 piece=▁bl\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1300 all=21139 active=1546 piece=▁same\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=33 min_freq=18\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1320 all=21207 active=1125 piece=iet\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1340 all=21351 active=1269 piece=▁name\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1360 all=21459 active=1377 piece=ets\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1380 all=21571 active=1489 piece=ively\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1400 all=21583 active=1501 piece=▁million\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=31 min_freq=17\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1420 all=21699 active=1196 piece=lymp\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1440 all=21745 active=1242 piece=▁Pakistan\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1460 all=21839 active=1336 piece=▁par\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1480 all=21907 active=1404 piece=▁collabor\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1500 all=22055 active=1552 piece=ugby\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28 min_freq=15\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1520 all=22104 active=1149 piece=▁quest\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1540 all=22186 active=1231 piece=ead\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1560 all=22288 active=1333 piece=ament\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1580 all=22345 active=1390 piece=▁consid\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1600 all=22380 active=1425 piece=but\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26 min_freq=14\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1620 all=22535 active=1269 piece=oints\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1640 all=22591 active=1325 piece=▁using\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1660 all=22608 active=1342 piece=val\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1680 all=22720 active=1454 piece=▁2021,\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1700 all=22727 active=1461 piece=▁position\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=25 min_freq=14\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1720 all=22898 active=1307 piece=ours\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1740 all=22985 active=1394 piece=▁inform\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1760 all=22991 active=1400 piece=AA\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1780 all=23138 active=1547 piece=▁sit\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1800 all=23171 active=1580 piece=▁trees\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=23 min_freq=13\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1820 all=23249 active=1237 piece=ety\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1840 all=23318 active=1306 piece=duced\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1860 all=23353 active=1341 piece=▁chart\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1880 all=23389 active=1377 piece=avy\ntrainer_interface.cc(687) LOG(INFO) Saving model: model_bpe.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: model_bpe.vocab\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"sp = spm.SentencePieceProcessor()\nsp.Load(\"/kaggle/working/model_bpe.model\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGPFzl8O86uC","executionInfo":{"status":"ok","timestamp":1748417092824,"user_tz":-480,"elapsed":20,"user":{"displayName":"なんさか（宅魚）","userId":"15021489365147789524"}},"outputId":"c7f3ebbd-cad5-4e6d-b805-6c694ff55576","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:06.993741Z","iopub.execute_input":"2025-05-29T00:45:06.994036Z","iopub.status.idle":"2025-05-29T00:45:07.012521Z","shell.execute_reply.started":"2025-05-29T00:45:06.994011Z","shell.execute_reply":"2025-05-29T00:45:07.012008Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"vocab_size = sp.GetPieceSize()","metadata":{"id":"HbhIOO_2_uJ0","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.014784Z","iopub.execute_input":"2025-05-29T00:45:07.015030Z","iopub.status.idle":"2025-05-29T00:45:07.029358Z","shell.execute_reply.started":"2025-05-29T00:45:07.015014Z","shell.execute_reply":"2025-05-29T00:45:07.028849Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"encoded_text = sp.EncodeAsIds(text)\ndata = torch.tensor(encoded_text, dtype=torch.long)","metadata":{"id":"TZECWMV_9Jjs","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.030082Z","iopub.execute_input":"2025-05-29T00:45:07.030319Z","iopub.status.idle":"2025-05-29T00:45:07.547164Z","shell.execute_reply.started":"2025-05-29T00:45:07.030292Z","shell.execute_reply":"2025-05-29T00:45:07.546543Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Split train and test set","metadata":{"id":"a4jKpVpZ6hVn"}},{"cell_type":"code","source":"n = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"id":"8cxuLxwjFS1r","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.547856Z","iopub.execute_input":"2025-05-29T00:45:07.548075Z","iopub.status.idle":"2025-05-29T00:45:07.578750Z","shell.execute_reply.started":"2025-05-29T00:45:07.548058Z","shell.execute_reply":"2025-05-29T00:45:07.578003Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Data Loader","metadata":{"id":"imMKQt17-AYO"}},{"cell_type":"code","source":"def get_batch(split):\n    '''\n    A function that returns a data batch for training for any given split (train or validation).\n    '''\n    data = train_data if split == 'train' else val_data\n    # Here we create 4 starting indexes (equal to batch_size) of the 4 data batches we want to sample\n    index_x = torch.randint(len(data) - block_size, (batch_size,))\n    # Get the context data\n    x = torch.stack([data[i : i+block_size] for i in index_x])\n    # Get our targets\n    y = torch.stack([data[i+1 : i+block_size+1] for i in index_x])\n    # Move data to device\n    x, y = x.to(device), y.to(device)\n    return x,y","metadata":{"id":"xn28AS5zFdh8","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.579554Z","iopub.execute_input":"2025-05-29T00:45:07.579786Z","iopub.status.idle":"2025-05-29T00:45:07.598992Z","shell.execute_reply.started":"2025-05-29T00:45:07.579766Z","shell.execute_reply":"2025-05-29T00:45:07.598195Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Loss Estimater","metadata":{"id":"OVYlEyrY-DJ5"}},{"cell_type":"code","source":"# This tells torch that it doesn't need to store the intermediate values as we will be doing no backpropagation.\n# Saves a lot of memory.\n@torch.no_grad()\ndef estimate_loss(model):\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"id":"-FV0LLh-Fsb3","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.600014Z","iopub.execute_input":"2025-05-29T00:45:07.600302Z","iopub.status.idle":"2025-05-29T00:45:07.626350Z","shell.execute_reply.started":"2025-05-29T00:45:07.600286Z","shell.execute_reply":"2025-05-29T00:45:07.625838Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Self Attention Head","metadata":{"id":"e4qrTWua-KRf"}},{"cell_type":"code","source":"class Head(nn.Module):\n    '''A class that represents a single SA head'''\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * C**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v=self.value(x)\n        out = wei @ v\n        return out","metadata":{"id":"duNoUEgOGGW8","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.626991Z","iopub.execute_input":"2025-05-29T00:45:07.627215Z","iopub.status.idle":"2025-05-29T00:45:07.643568Z","shell.execute_reply.started":"2025-05-29T00:45:07.627199Z","shell.execute_reply":"2025-05-29T00:45:07.643089Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Multi-Head Attention","metadata":{"id":"nL3l4CBv-l50"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n  '''multiple heads of self-attention in parallel'''\n\n  def __init__(self, num_heads, head_size):\n    super().__init__()\n    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n    self.proj = nn.Linear(head_size*num_heads, n_embd)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    out = torch.cat([h(x) for h in self.heads], dim=-1)\n    out = self.dropout(self.proj(out))\n    return out","metadata":{"id":"Tbbu11SEGmZs","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.644204Z","iopub.execute_input":"2025-05-29T00:45:07.644429Z","iopub.status.idle":"2025-05-29T00:45:07.662287Z","shell.execute_reply.started":"2025-05-29T00:45:07.644410Z","shell.execute_reply":"2025-05-29T00:45:07.661778Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Feed Forward","metadata":{"id":"zo7UDUfM-rog"}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n  '''a linear layer followed by a non-linearity'''\n  def __init__(self, n_embd):\n    super().__init__()\n    self.net = nn.Sequential(\n        nn.Linear(n_embd, 4 * n_embd),\n        nn.ReLU(),\n        nn.Linear(4 * n_embd, n_embd),\n        nn.Dropout(dropout),\n    )\n\n  def forward(self, x):\n    return self.net(x)","metadata":{"id":"8OpE6kU0Hqmr","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.662943Z","iopub.execute_input":"2025-05-29T00:45:07.663184Z","iopub.status.idle":"2025-05-29T00:45:07.692097Z","shell.execute_reply.started":"2025-05-29T00:45:07.663170Z","shell.execute_reply":"2025-05-29T00:45:07.691536Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Layer Blocks","metadata":{"id":"fIVZAGQS-yZx"}},{"cell_type":"code","source":"class Block(nn.Module):\n  '''A Transformer block: communication followed by computation'''\n  def __init__(self, n_embd, n_head):\n    super().__init__()\n    head_size = n_embd // n_head\n    self.sa = MultiHeadAttention(n_head, head_size)\n    self.ffwd = FeedForward(n_embd)\n    self.ln1 = nn.LayerNorm(n_embd)\n    self.ln2 = nn.LayerNorm(n_embd)\n\n  def forward(self, x):\n    x = x + self.sa(self.ln1(x))\n    x = x + self.ffwd(self.ln2(x))\n    return x","metadata":{"id":"PW2DNk1lHzbg","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.692700Z","iopub.execute_input":"2025-05-29T00:45:07.692977Z","iopub.status.idle":"2025-05-29T00:45:07.710360Z","shell.execute_reply.started":"2025-05-29T00:45:07.692961Z","shell.execute_reply":"2025-05-29T00:45:07.709905Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Final Model","metadata":{"id":"Gq3v2RU4_QZJ"}},{"cell_type":"code","source":"class GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        print(\"Created the GPTLanguageModel\")\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size) # Language model head\n\n    # This is not a feed forward layer, but gets us the next logits we need for the generate method\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n\n        logits = self.lm_head(x)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    # A method that will generate the next token in our timeline. So like:\n    # \"A ca\" -> \"A cat\" -> \"A cat \" -> ...\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            # Get the current predictions. This \"self\" call will automatically call\n            # our \"forward\" method above\n            logits, loss = self(idx_cond)\n            # focus only on the last time step (token)\n            logits = logits[:, - 1, :]\n            # Do a softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)\n            # Sample from the prob distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # Append the next probable index (=token) to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx","metadata":{"id":"iEAGop_bIB_b","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.711020Z","iopub.execute_input":"2025-05-29T00:45:07.711216Z","iopub.status.idle":"2025-05-29T00:45:07.726500Z","shell.execute_reply.started":"2025-05-29T00:45:07.711202Z","shell.execute_reply":"2025-05-29T00:45:07.726020Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = GPTLanguageModel()\nm = model.to(device)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7SwIBo5JOdd","executionInfo":{"status":"ok","timestamp":1748417093815,"user_tz":-480,"elapsed":220,"user":{"displayName":"なんさか（宅魚）","userId":"15021489365147789524"}},"outputId":"74e31d32-2f77-4d17-f6d1-9751c8ac3f9f","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:07.727120Z","iopub.execute_input":"2025-05-29T00:45:07.727612Z","iopub.status.idle":"2025-05-29T00:45:08.174633Z","shell.execute_reply.started":"2025-05-29T00:45:07.727588Z","shell.execute_reply":"2025-05-29T00:45:08.173962Z"}},"outputs":[{"name":"stdout","text":"Created the GPTLanguageModel\n12.276944 M parameters\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Model Training","metadata":{"id":"-0XvFRtF_aIs"}},{"cell_type":"code","source":"def save_checkpoint(\n  model: GPTLanguageModel,\n  optimizer: torch.optim.Optimizer,\n  epoch: int,\n  loss: float,\n  path: str = \"checkpoint.pth\"\n) -> None:\n  checkpoint = {\n      'epoch': epoch,\n      'model_state_dict': model.state_dict(),\n      'optimizer_state_dict': optimizer.state_dict(),\n      'loss': loss\n  }\n  torch.save(checkpoint, path)","metadata":{"id":"XSDUXBFEw71e","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:08.175343Z","iopub.execute_input":"2025-05-29T00:45:08.175611Z","iopub.status.idle":"2025-05-29T00:45:08.179641Z","shell.execute_reply.started":"2025-05-29T00:45:08.175588Z","shell.execute_reply":"2025-05-29T00:45:08.179060Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n  if iter % eval_interval == 0 or iter == max_iters - 1:\n    losses = estimate_loss(model)\n    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n  xb, yb = get_batch('train')\n\n  logits, loss = model.forward(xb, yb)\n  optimizer.zero_grad(set_to_none=True)\n  loss.backward()\n  optimizer.step()\n\n  if iter+1 == max_iters:\n    save_checkpoint(model, optimizer, iter+1, loss.item())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PB0WmsZyJ3i-","executionInfo":{"status":"ok","timestamp":1748420453628,"user_tz":-480,"elapsed":3359787,"user":{"displayName":"なんさか（宅魚）","userId":"15021489365147789524"}},"outputId":"6fd73a84-319f-423b-fb98-1c57dd9900c2","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:45:08.180245Z","iopub.execute_input":"2025-05-29T00:45:08.180476Z","iopub.status.idle":"2025-05-29T01:44:12.333547Z","shell.execute_reply.started":"2025-05-29T00:45:08.180455Z","shell.execute_reply":"2025-05-29T01:44:12.332958Z"}},"outputs":[{"name":"stdout","text":"step 0: train loss 7.7714, val loss 7.7628\nstep 500: train loss 3.7497, val loss 5.1306\nstep 1000: train loss 2.1897, val loss 5.4688\nstep 1500: train loss 0.8791, val loss 6.3776\nstep 2000: train loss 0.3414, val loss 7.2479\nstep 2500: train loss 0.1984, val loss 7.9485\nstep 3000: train loss 0.1504, val loss 8.4284\nstep 3500: train loss 0.1241, val loss 8.7986\nstep 4000: train loss 0.1088, val loss 9.1521\nstep 4500: train loss 0.0987, val loss 9.3875\nstep 4999: train loss 0.0907, val loss 9.6029\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"prompt = \"She was a daughter of\"\ncontext = torch.tensor(sp.EncodeAsIds(prompt), dtype=torch.long, device=device).unsqueeze(0)\nprint(sp.DecodeIds(m.generate(context, max_new_tokens=100)[0].tolist()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b25UetUZCxtA","executionInfo":{"status":"ok","timestamp":1748422006904,"user_tz":-480,"elapsed":2009,"user":{"displayName":"なんさか（宅魚）","userId":"15021489365147789524"}},"outputId":"17a2e472-8fa3-41c3-8275-ff6c0f9eb55f","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T01:44:12.334547Z","iopub.execute_input":"2025-05-29T01:44:12.334877Z","iopub.status.idle":"2025-05-29T01:44:15.007376Z","shell.execute_reply.started":"2025-05-29T01:44:12.334859Z","shell.execute_reply":"2025-05-29T01:44:15.006394Z"}},"outputs":[{"name":"stdout","text":"She was a daughter of Francisanillan Wrgicol. She began try his Royal Ne lands by of Amaovichol. Maria, a  ⁇ BL. Journal of Nar Hopeaen (18 D. S. In 1922. In 1947, Mahoney and Thalia Mara became managers in the first place of Dance. They became artistic directors that year, with the marathals Ji\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Closure\n\nObviously, there is overfit. The model is trained by a small dataset. The validation dataset is small, and it may contain different topics to the train dataset. The purpose of this step is learning the language. To see the learning result, we finally generate some text. This text makes sense. So we can say that this model works.","metadata":{"id":"Q1BnziPKPHtx"}}]}