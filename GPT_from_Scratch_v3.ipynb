{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGwYAXHN4LFbkdf98gr/xv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zjzhao1002/GPT-from-Scratch/blob/main/GPT_from_Scratch_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Modules"
      ],
      "metadata": {
        "id": "zSVCDPYT4xG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "N0ujKfrx_Q9-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparemeters"
      ],
      "metadata": {
        "id": "zWLB1Hk_6PzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a random seed\n",
        "torch.manual_seed(1337)\n",
        "# How many independent sequences will we process in parallel?\n",
        "batch_size = 64\n",
        "# What is the maximum context length for prediction?\n",
        "block_size = 256\n",
        "# How many iterations we will be doing in our training loop\n",
        "max_iters = 5000\n",
        "# The interval in which we want to calculate the loss. We cannot do that after each step\n",
        "eval_interval = 500\n",
        "# The learning rate of the model\n",
        "learning_rate = 3e-4\n",
        "# Use GPU to train the model. If GPU is not existed, use the CPU instead.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# The amount of iterations we use in our loss function.\n",
        "eval_iters = 200\n",
        "# Number of Embedding dimensions\n",
        "n_embd = 384\n",
        "# Number of Heads\n",
        "n_head = 6\n",
        "# Number of Block Layers\n",
        "n_layer = 6\n",
        "# Dropout\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "56PiCAchCZll"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "mTEgq2G86cR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/questions.txt\", 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "MJLh07VmEy51"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.Train(\n",
        "    input = \"/content/questions.txt\",\n",
        "    model_prefix = \"model_bpe\",\n",
        "    vocab_size = 2000,\n",
        "    model_type = \"bpe\"\n",
        ")"
      ],
      "metadata": {
        "id": "6wKUSV0R7I2x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"model_bpe.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGPFzl8O86uC",
        "outputId": "b34755a8-adfc-4390-f827-ffdabf3918c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = sp.GetPieceSize()"
      ],
      "metadata": {
        "id": "HbhIOO_2_uJ0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = sp.EncodeAsIds(text)\n",
        "data = torch.tensor(encoded_text, dtype=torch.long)"
      ],
      "metadata": {
        "id": "TZECWMV_9Jjs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split train and test set"
      ],
      "metadata": {
        "id": "a4jKpVpZ6hVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "8cxuLxwjFS1r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "imMKQt17-AYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    '''\n",
        "    A function that returns a data batch for training for any given split (train or validation).\n",
        "    '''\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # Here we create 4 starting indexes (equal to batch_size) of the 4 data batches we want to sample\n",
        "    index_x = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    # Get the context data\n",
        "    x = torch.stack([data[i : i+block_size] for i in index_x])\n",
        "    # Get our targets\n",
        "    y = torch.stack([data[i+1 : i+block_size+1] for i in index_x])\n",
        "    # Move data to device\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "xn28AS5zFdh8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Estimater"
      ],
      "metadata": {
        "id": "OVYlEyrY-DJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This tells torch that it doesn't need to store the intermediate values as we will be doing no backpropagation.\n",
        "# Saves a lot of memory.\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "-FV0LLh-Fsb3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention Head"
      ],
      "metadata": {
        "id": "e4qrTWua-KRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    '''A class that represents a single SA head'''\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v=self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "duNoUEgOGGW8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention"
      ],
      "metadata": {
        "id": "nL3l4CBv-l50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  '''multiple heads of self-attention in parallel'''\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(head_size*num_heads, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "Tbbu11SEGmZs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed Forward"
      ],
      "metadata": {
        "id": "zo7UDUfM-rog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  '''a linear layer followed by a non-linearity'''\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "8OpE6kU0Hqmr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer Blocks"
      ],
      "metadata": {
        "id": "fIVZAGQS-yZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  '''A Transformer block: communication followed by computation'''\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "PW2DNk1lHzbg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model"
      ],
      "metadata": {
        "id": "Gq3v2RU4_QZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Created the GPTLanguageModel\")\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # Language model head\n",
        "\n",
        "    # This is not a feed forward layer, but gets us the next logits we need for the generate method\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # A method that will generate the next token in our timeline. So like:\n",
        "    # \"A ca\" -> \"A cat\" -> \"A cat \" -> ...\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get the current predictions. This \"self\" call will automatically call\n",
        "            # our \"forward\" method above\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step (token)\n",
        "            logits = logits[:, - 1, :]\n",
        "            # Do a softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Sample from the prob distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # Append the next probable index (=token) to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "iEAGop_bIB_b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7SwIBo5JOdd",
        "outputId": "2cdc74cd-240f-45cc-af6a-61920477f6fd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created the GPTLanguageModel\n",
            "12.276944 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "-0XvFRtF_aIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss(model)\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model.forward(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB0WmsZyJ3i-",
        "outputId": "79b8970a-4d52-4ab3-c15f-72695ace691d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 7.7675, val loss 7.7561\n",
            "step 500: train loss 0.0879, val loss 9.1053\n",
            "step 1000: train loss 0.0234, val loss 10.1862\n",
            "step 1500: train loss 0.0177, val loss 10.6859\n",
            "step 2000: train loss 0.0160, val loss 11.0797\n",
            "step 2500: train loss 0.0154, val loss 11.3037\n",
            "step 3000: train loss 0.0150, val loss 11.4353\n",
            "step 3500: train loss 0.0148, val loss 11.6594\n",
            "step 4000: train loss 0.0150, val loss 11.8339\n",
            "step 4500: train loss 0.0147, val loss 11.8958\n",
            "step 4999: train loss 0.0146, val loss 12.0190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(sp.DecodeIds(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBRq1yYPXC3C",
        "outputId": "b8c5c0cc-84d1-46f4-e456-a7c6fbec87cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ⁇ lazy learner ⁇  or  ⁇ memory based ⁇  model too. KNN relies on the principle that similar data points tend to belong to the same class or have similar target values. This means that, In the training phase, KNN stores the entire dataset consisting of feature vectors and their corresponding class labels (for classification) or target values (for regression). It then calculates the distances between that point and all the points in the training dataset. (commonly used distance metrics are Euclidean distance and Manhattan distance). (Note : Choosing an appropriate value for k is crucial. A small k may result in noisy predictions, while a large k can smooth out the decision boundaries. The choice of distance metric and feature scaling also impact KNN’s performance.) Question: What is the Naïve Bayes algorithm, what are the different assumptions of Naïve Bayes? Answer: The Naïve Bayes algorithm is a probabilistic classification algorithm based on Bayes theorem with a \"naïve\" assumption of feature independence within each class. It is commonly used for both binary and multi-class classification tasks, particularly in situations where simplicity, speed, and efficiency are essential. The main assumptions that Naïve Bayes theorem makes are: * Feature independence – It assumes that the features involved in Naïve Bayes algorithm are conditionally independent, i.e., the presence / absence of one feature does not affect any other feature. * Equality – This assumes that the features are equal in terms of importance (or weight). * Normality – It assumes that the feature distribution is Normal in nature, i.e., the data is distributed equally around its mean. Question: What are decision trees, and how do they work? Answer: Decision trees are a popular machine learning algorithm used for both classification and regression tasks. They work by creating a tree-like structure of decisions based on input features to make predictions or decisions. Lets dive into its core concepts and how they work briefly: * Decision trees consist of nodes and edges. * The tree starts with a root node and branches into internal nodes that represent features or attributes. * These nodes contain decision rules that split the data into subsets. * Edges connect nodes and indicate the possible decisions or outcomes. * Leaf nodes represent the final predictions or decisions. The objective is to increase data homogeneity, which is often measured using standards like mean squared error (for regression) or Gini impurity (for classification). Decision trees can handle a variety of attributes and can effectively capture complex data relationships. They can, however, overfit, especially when deep or complex. To reduce overfitting, strategies like pruning and restricting tree depth are applied. Question: Describe random forests and their advantages over single-decision trees. Answer: Random Forests are an ensemble learning technique that combines multiple decision trees to improve predictive accuracy and reduce overfitting. The advantages it has over single decision trees are: * Improved Generalization: Single decision trees are prone to overfitting, especially when they become deep and complex. Random Forests mitigate this issue by averaging predictions from multiple trees, resulting in a more generalized model that performs better on unseen data. * Better Handling of High-Dimensional Data : Random Forests are effective at handling datasets with a large number of features. They select a random subset of features for each tree, which can improve the performance when there are many irrelevant or noisy features. * Robustness to Outliers: Random Forests are more robust to outliers because they combine predictions from multiple trees, which can better handle extreme cases. Question: Explain the uniform distribution. Answer: A fundamental probability distribution in statistics is the uniform distribution, commonly referred to as the rectangle distribution. A constant probability density function (PDF) across a limited range characterises it. In simpler terms, in a uniform distribution, every value within a specified range has an equal chance of occurring. Question: Describe the Bernoulli distribution. Answer: A discrete probability distribution, the Bernoulli distribution is focused on discrete random variables. The number of heads you obtain while tossing three coins at once or the number of pupils in a class are examples of discrete random variables that have a finite or countable number of potential values. Question: What is the binomial distribution? Answer: The binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent Bernoulli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closure\n",
        "\n",
        "Obviously, there is overfit. The model is trained by a small dataset. The validation dataset is small, and it may contain different topics to the train dataset.\n",
        "\n",
        "Finally, we can see the model can generate some text. This text makes sense. So we can say that this model works."
      ],
      "metadata": {
        "id": "Q1BnziPKPHtx"
      }
    }
  ]
}